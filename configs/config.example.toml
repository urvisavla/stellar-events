# Stellar Events Configuration
# Copy this file to config.toml and modify as needed

# =============================================================================
# Source Data
# =============================================================================
[source]
# Path to ledger chunk files
ledger_dir = "./data/ledgers"

# Network: "mainnet", "testnet", or custom passphrase
network = "mainnet"

# =============================================================================
# Storage (RocksDB)
# =============================================================================
[storage]
# Path to RocksDB database directory
db_path = "./rocksdb/events"

# --- Write Performance ---
# Memtable size in MB (larger = better write throughput, more memory)
write_buffer_size_mb = 256

# Number of memtables to keep in memory
max_write_buffer_number = 4

# Minimum memtables to merge before flushing
min_write_buffer_number_to_merge = 2

# --- Read Performance ---
# Block cache size in MB (larger = better read performance)
block_cache_size_mb = 512

# Bloom filter bits per key (10 is good balance, 0 to disable)
bloom_filter_bits_per_key = 10

# Cache index and filter blocks in block cache
cache_index_and_filter_blocks = true

# --- Background Jobs ---
# Parallel background threads for flushing memtables
max_background_jobs = 8

# --- Compression ---
# Compression algorithm: "none", "snappy", "lz4", "zstd"
compression = "lz4"

# Compression for bottommost level (oldest data)
bottommost_compression = "zstd"

# --- WAL (Write-Ahead Log) ---
# Disable WAL for faster bulk ingestion (safe - data can be re-ingested)
disable_wal = true

# --- Compaction ---
# Disable background compaction during ingestion
disable_auto_compaction = true

# Target SST file size in MB (larger = fewer files)
target_file_size_mb = 256

# Max bytes for level 1 in MB
max_bytes_for_level_base_mb = 1024

# =============================================================================
# Ingestion
# =============================================================================
[ingestion]
# Progress file path (empty = disabled)
# When set, writes progress JSON every 10k ledgers with:
# - ledger range being ingested
# - events processed, avg per ledger
# - processing rate and ETA
progress_file = "progress.json"

# --- Post-processing ---
# Run compaction after ingestion (optimizes for reads)
final_compaction = false

# Compute event stats after ingestion (slow)
compute_stats = false

# --- Index Maintenance ---
# Maintain L1 bitmap indexes during ingestion (fast queries)
bitmap_indexes = true

# Maintain unique value counts during ingestion
# If false, run 'build-indexes' command after ingestion
unique_indexes = false

# Ledgers between bitmap index flushes (prevents memory growth)
# Set to 0 to only flush at end of ingestion
bitmap_flush_interval = 10000

# --- Parallelism ---
# Number of parallel workers (0 = all CPUs)
workers = 0

# Ledgers to batch before writing
batch_size = 100

# Pipeline buffer size (0 = workers * 2)
queue_size = 0

# =============================================================================
# Query
# =============================================================================
[query]
# Max ledgers to scan if end not specified
max_ledger_range = 100000

# Default max events to return
default_limit = 100
